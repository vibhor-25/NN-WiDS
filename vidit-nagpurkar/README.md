Name: Vidit NagpurkarID: 25B12701. Learning Phase & Technical FoundationsEnvironment & ToolingGoogle Colab: Mastered the use of Google Colab and .ipynb files for cloud-based development.Core Python Libraries:NumPy: Utilized for high-performance n-dimensional array processing. This was critical to the actual project to allow simultaneous calculation of gradients across entire data sets instead of using slow Python loops.Pandas: Learnt to handle CSV files, including managing missing data and performing complex data encoding implementations.Matplotlib: Used to plot available data points for visualizing correlations between environment variables and crop yields.Scikit-Learn: Transitioned from manual implementations to industry-standard libraries. Studied the standard implementation for linear regression through gradient descent.Theoretical MasteryRegression Analysis: Studied Linear and Polynomial regression and understood the relationship between independent features and continuous target variables.Optimization Theory: Understood optimization through gradient descent to understand how models learn by minimizing cost functions.2. Implementation Logic (Week 2)Optimization StrategyImplementation, specifically in week-2, centers on a custom implementation of Linear Regression. Rather than using high-level libraries, optimization was done using a from-scratch implementation of Vectorized Gradient Descent.Feature Engineering & PreprocessingNumerical Scaling: Data in the form of text like "Furnishing Status" and "Floor Level" were converted into numerical scales to be able to feed into the Linear Regression Model.Target Encoding: Implemented for encoding the "city" variable in a rent-calculation model to be able to effectively capture geographic trends.Z-Score Normalization: Implemented to ensure fast convergence. All features were standardized to a mean of 0 and standard deviation of 1.Model Expansion & EvaluationPolynomial Expansion: For the non-linear dataset, Polynomial Expansion was implemented to be able to use the linear regression model for fitting the given quadratic curve.Evaluation Metrics: Models were evaluated using metrics like Root-Mean Square Error (RMSE) and Mean-Absolute Error (MAE).3. Challenges OvercomeA. Vanishing and Exploding GradientsA challenge in the "House Rent" model was the scale of the data. Initially, when I ran my original model, it returned NaN or infinity because the "Size" feature caused my gradient updates to be very large. To counter this, I implemented Z-score standardization to scale all features to a mean of 0 and standard deviation of 1.B. Heuristic Data TransformationThe provided "Floor" data text was quite unstructured. I engineered a custom parsing function that extracted floor number and total floors data from the irregular strings. Categorical labels like "Ground" and "Basement" were found and converted into numerical values. This also allowed me to generate a "floor ratio" feature for the linear regression model, allowing for higher accuracy.C. Learning Rate ProblemsThough the initial implementation was simple, the learning rate proved to be a difficult value to tune for a custom-made model. The value of $10^{-2}$ that worked for the fruit harvest model did not work at all for the rent model and a much finer learning rate was required. Making the learning rate too fine led to the model being run for several minutes without improved results. Finding the exact value for the learning rate that worked was quite a slow process for larger data sets.Conclusion: Solving these issues provided to me a deeper insight into why preprocessing is much more important than the model itself.Would you like me to help you create a "Results" table to summarize the final accuracy or error values from your code?
